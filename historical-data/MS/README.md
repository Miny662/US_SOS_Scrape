# ğŸ•µï¸ Business ID Scraper for Mississippi Secretary of State (SOS)

This project scrapes business registration data from the Mississippi Secretary of State website using direct API requests.

It supports:
- ğŸ” Concurrent scraping with **100 rotating proxies**
- ğŸ“Œ Automatic **progress tracking and resuming**
- ğŸ§¹ Data **cleaning** and **structured JSONL** output
- ğŸ“¦ Merging partial data into a **single, sorted dataset**
- ğŸ§¼ **Auto-cleanup** of intermediate files after completion

---

## ğŸ“Œ Features

- ğŸ”„ **Concurrent scraping** using 100 Webshare proxies
- ğŸ“ˆ **Workload distributed evenly** across proxies
- â™»ï¸ **Progress saved per proxy**, allowing resume on failure
- ğŸ§½ **Cleaned data** output (removes whitespace/formatting noise)
- ğŸ“‘ **Merges all partial files** into one final sorted `JSONL`
- ğŸ—‘ï¸ **Deletes** all intermediate files after final merge

---

## ğŸ§° Requirements

- Python **3.8+**
- Dependencies listed in [`requirements.txt`](./requirements.txt)

---

## âš™ï¸ Setup Instructions

1. Add your proxy file
Place your proxy list in the root directory as:

Webshare 1000 proxies - option 1.txt
Each line should follow this format:

host:port:user:password

2. (Optional) Create a virtual environment

python -m venv venv
source venv/bin/activate  # Windows: venv\Scripts\activate

3. Install dependencies

pip install -r requirements.txt

â–¶ï¸ Usage
Run the script:


python scraper.py
The script will:

ğŸ”€ Select 100 random proxies from your list

ğŸ§¾ Scrape business data for IDs from 650001 to 1,114,681

ğŸ’¾ Save per-proxy JSONL files in Output/

âœ… Track progress and resume automatically if interrupted

ğŸ“‚ Merge all partial files into Output/merged_data.jsonl

ğŸ§¹ Delete all progress and partial files after merging